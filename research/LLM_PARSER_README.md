# LLM-Парсер документов

Этот скрипт использует Ollama для извлечения структурированных данных из текстовых файлов с помощью пользовательских промптов.

## Особенности

- ✅ **Двухэтапная обработка** - сначала разбивка на блоки, затем специализированная обработка
- ✅ **Модульные промпты** - отдельные файлы для каждого типа данных
- ✅ **Промежуточные файлы** - сохранение результатов разбивки в `.tmp.json`
- ✅ **Поддержка Ollama** - работает с локальными и удаленными моделями
- ✅ **Гибкая настройка** - каталог промптов, модель, URL сервера

## Установка Ollama

```bash
# Установка Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Запуск сервера
ollama serve

# Установка модели (например, llama3.2)
ollama pull llama3.2
```

## Использование

### Базовое использование
```bash
python3 llm_parser.py --file "output/01-0530_6856.txt" --model llama3.2
```

### С настройками
```bash
python3 llm_parser.py \
  --file "output/01-0530_6856.txt" \
  --prompt-dir "prompts" \
  --model llama3.2 \
  --output "result.json"
```

### С удаленным Ollama
```bash
python3 llm_parser.py \
  --file "output/01-0530_6856.txt" \
  --model llama3.2 \
  --ollama-url "http://192.168.1.100:11434" \
  --token "your-token"
```

## Параметры

- `--file, -f` - путь к текстовому файлу (обязательно)
- `--prompt-dir, -p` - каталог с файлами промптов (по умолчанию: prompts)
- `--output, -o` - выходной JSON файл (по умолчанию: имя_файла.json)
- `--ollama-url` - URL Ollama сервера (по умолчанию: http://localhost:11434)
- `--model, -m` - название модели (обязательно)
- `--token, -t` - токен авторизации (опционально)
- `--no-ssl-verify` - отключить проверку SSL сертификата

## Ограничения и решения

### 1. Размер контекста модели
- **Проблема**: модели имеют ограничение на количество токенов
- **Решение**: автоматическое разбиение на чанки с перекрытием

### 2. Размер промпта
- **Проблема**: длинный промпт + длинный чанк = превышение лимита
- **Решение**: автоматическое разбиение промпта на части

### 3. Производительность
- **Проблема**: много запросов к API
- **Решение**: паузы между запросами, батчинг

## Рекомендуемые модели

- **llama3.2** - хороший баланс качества и скорости
- **llama3.1** - более качественная, но медленнее
- **qwen2.5** - отлично работает с русским текстом
- **mistral** - быстрая и эффективная

## Пример промпта

Создайте файл `prompt.txt` с инструкциями для модели:

```
Извлеки структурированные данные из технического документа о производстве стали.

Верни результат в формате JSON со следующей структурой:
{
  "document_info": {
    "document_number": "номер документа",
    "date": "дата документа"
  },
  "steel_info": {
    "steel_grade": "марка стали"
  }
}

ВАЖНО: возвращай только валидный JSON без дополнительного текста.
```

## Структура проекта

```
letterexplorer/
├── prompts/                    # Каталог с промптами
│   ├── prompt_main.txt         # Основной промпт для разбивки
│   ├── prompt_document_info.txt # Промпт для базовой информации
│   ├── prompt_document_title.txt # Промпт для заголовка
│   ├── prompt_steel_info.txt   # Промпт для информации о стали
│   ├── prompt_chemical_composition.txt # Промпт для хим. состава
│   ├── prompt_technical_requirements.txt # Промпт для тех. требований
│   ├── prompt_tables.txt       # Промпт для таблиц
│   └── README.md               # Документация промптов
├── llm_parser.py              # Основной скрипт
├── input/                     # Входные файлы
├── output/                    # Выходные файлы
└── README.md                  # Документация
```

## Структура результата

```json
{
  "file_info": {
    "filename": "01-0530_6856.txt",
    "processing_method": "two_stage_separate_prompts",
    "total_prompts": 7
  },
  "extracted_data": {
    "document_info": {...},
    "document_title": {...},
    "steel_info": {...},
    "chemical_composition": {...},
    "technical_requirements": {...},
    "tables": {...}
  }
}
```

## Отладка

Скрипт автоматически:
- Тестирует соединение с Ollama
- Показывает информацию о модели
- Выводит прогресс обработки
- Сохраняет результаты по чанкам
